#!/usr/bin/python
# -*- coding: utf-8 -*-
'''
# reportGLAMdashboard.py

A generalized tool based on Fae's experience of reporting needs
and informal discussion with the GLAM/Commons community
http://commons.wikimedia.org/wiki/User:Faebot/GLAM_dashboard

There is poorly designed and dead code here! You can have quick and dirty
or clean and never. It's the wiki-way, probably. ;-)

Date: November 2014
Author: Fae, http://commons.wikimedia.org/wiki/User:Fae
Permissions: CC-BY-SA-4.0

This tool was produced by Fae as an unpaid volunteer, without support
from any organization and should not be claimed to be part of any sponsored
project.
'''

# import urllib
import pywikibot, re
from pywikibot import pagegenerators
from pywikibot.compat import catlib
import MySQLdb

REQUEST_PAGE = 'User:Faebot/GLAM dashboard'
USUAL_BADCATS = ["Files_from",
                 "CC-",
                 "Files_with",
                 "test_",
                 "Uploaded_with",
                 "Self-pub",
                 "Items_with_OTRS",
                 "GFDL",
                 "FAL",
                 "PD-",
                 "uploaded_by_",
                 "Flickr_images_reviewed",
                 "Flickr_images_uploaded"
                 ]

def improvement(cat_list):
		cursor.execute("""
SELECT
	page_title,
	CONCAT(REPLACE(gil_wiki,'wiki',''), ':',  gil_page_title) AS link
FROM page
INNER JOIN globalimagelinks ON page_title = gil_to
INNER JOIN categorylinks ON page_id=cl_from AND cl_to IN (""" + cat_list + """)
INNER JOIN image ON img_name=page_title
WHERE
	gil_page_namespace=""
	AND RIGHT(gil_wiki,4)='wiki'
	AND gil_wiki!='wikidata'
GROUP BY page_title
HAVING COUNT(gil_page_title)=1
ORDER BY RAND()
LIMIT 10;
		""")
		gallery=[]
		for page, link in cursor.fetchall():
				gallery.append("File:"+page +"|<center>[[:"+ re.sub('_',' ',re.sub('data:','wikidata:', link)) +"]]</center>")
		report = "Ten randomly selected files with a single mainspace use on Wikimedia projects:\n<gallery>\n"+"\n".join(gallery)+"\n</gallery>"
		pywikibot.output("Working out mincat")
		cursor.execute("""
				SELECT COUNT(c4.cl_to)
				FROM page p2
				JOIN categorylinks c3 ON page_id=c3.cl_from
				JOIN categorylinks c4 ON page_id=c4.cl_from
				WHERE c4.cl_to IN (""" + cat_list + """)
				AND p2.page_namespace=6
				GROUP BY p2.page_title
				ORDER BY COUNT(c4.cl_to)
				LIMIT 1;""")
		for c in cursor.fetchall():
				mincats = c[0]
		# print "*** mincats is", int(float(mincats))
		mincats = int(float(mincats))
		cursor.execute("""
SELECT p1.page_title, page_id
FROM page p1
JOIN categorylinks c1 ON p1.page_id=c1.cl_from
JOIN categorylinks c2 ON p1.page_id=c2.cl_from
WHERE
c1.cl_to IN (""" + cat_list + """)
AND page_namespace=6
GROUP BY p1.page_title
HAVING COUNT(c2.cl_to)="""+str(mincats)+""" LIMIT 10;""")
		gallery=[]
		for page, page_id in cursor.fetchall():
				gallery.append("File:"+page +"\n")
		report += "\nUp to ten randomly selected files with the lowest category counts in the project:\n<gallery>\n"+"\n".join(gallery)+"\n</gallery>"
		return report

def most_edited(cat_list):
		cursor.execute("""SELECT DISTINCT page_title, (SELECT COUNT(rev_timestamp) FROM revision WHERE page_id=rev_page) AS Edits
		FROM page
		JOIN categorylinks ON page_id=cl_from
		JOIN revision ON page_id=rev_page
		WHERE cl_to IN (""" + cat_list + """)
			AND page_namespace=6
		ORDER BY (SELECT COUNT(rev_timestamp) FROM revision WHERE page_id=rev_page) DESC
		LIMIT 24;""")

		table = []
		for page_title, edits in cursor.fetchall():
			table.append("File:"+page_title+"|<center>"+ str(edits) +" edits</center>")

		table = "<gallery>\n"+"\n".join(table) +"\n</gallery>"
		return table

def largest(cat_list):
		cursor.execute("""
		SELECT CONCAT("File:",img_name) AS File,
				 CONCAT( ROUND(img_width*img_height/1000000), " MP<br><small>", img_width, "x", img_height, " pixels</small>" ) AS Size
				 FROM image
				 JOIN page ON page_title=img_name
				 JOIN categorylinks ON page_id=cl_from
				 WHERE cl_to IN (""" + cat_list+""")
				 AND img_width>0
				 ORDER BY img_width*img_height DESC
				 LIMIT 24;
		""")

		table = []
		for File, Size in cursor.fetchall():
			table.append(File+"|<center>"+Size+"</center>")
		table="<gallery>\n"+"\n".join(table)+"\n</gallery>"
		return table

'''
SELECT CONCAT("File:",img_name) AS File,
CONCAT( ROUND(img_width*img_height/1000000), " MP<br><small>", img_width, "x", img_height, " pixels</small>" ) AS Size
FROM image
JOIN page ON page_title=img_name
JOIN categorylinks ON page_id=cl_from
WHERE cl_to IN ("Xeno-canto")
ORDER BY img_width*img_height DESC
LIMIT 24;
'''

def volunteers(cat_list):
		cursor.execute("""
		SELECT DISTINCT rev_user_text,
			COUNT(page_title) AS edit_count
		FROM categorylinks
		RIGHT JOIN page ON cl_from = page_id
		LEFT JOIN revision ON page_id = rev_page
		WHERE page_namespace=6
			AND cl_to IN (""" + cat_list + """)
			AND rev_user_text NOT REGEXP '[Bb]ot'
		GROUP BY 1
		HAVING edit_count>0
		ORDER BY 2 DESC;"""
		)

		table = []
		for userx, count in cursor.fetchall():
				table.append([count, "<abbr title='{} edits'>[[User talk:".format(count) + userx + "|" +userx +"]]</abbr>"])

		result = "{| class='wikitable sortable'\n!Edits!!#!!Volunteers"
		r = [u[1] for u in table if u[0]>999]
		if len(r)>0:
				result += "\n|-\n|1000+||" + str(len(r)) +"||"+"{{middot}}".join(r)
		r = [u[1] for u in table if u[0]>99 and u[0]<1000]
		if len(r)>0:
				result += "\n|-\n|100+ ||" + str(len(r)) +"||"+"{{middot}}".join(r)
		r = [u[1] for u in table if u[0]>9 and u[0]<100]
		result += "\n|-\n|10+  ||" + str(len(r)) +"||"+"{{middot}}".join(r)
		r = [u[1] for u in table if u[0]<10]
		result += "\n|-\n|     ||" + str(len(r)) +"||"+"{{middot}}".join(r)
		result += "\n|}"

		return result
'''
SELECT DISTINCT rev_user_text,
COUNT(page_title) AS edit_count
FROM categorylinks
RIGHT JOIN page ON cl_from = page_id
LEFT JOIN revision ON page_id = rev_page
WHERE page_namespace=6
AND cl_to IN ('Media_contributed_by_Zentralbibliothek_Solothurn')
AND rev_user_text NOT REGEXP '[Bb]ot'
GROUP BY 1
HAVING edit_count>0
ORDER BY 2 DESC;
'''

def glamorous_list(cat_list):
		query = """
SELECT
	page_title,
	count(gil_page_title) AS gilcount,
	GROUP_CONCAT(CONCAT(gil_wiki,' : ',IF(gil_page_namespace!='',CONCAT(gil_page_namespace, ':'),gil_page_title)) SEPARATOR ' // ') AS list FROM page
JOIN categorylinks ON page_id=cl_from
JOIN globalimagelinks ON page_title=gil_to
WHERE cl_to IN ("""+cat_list+""")
GROUP BY page_title
ORDER BY gilcount DESC
LIMIT 24;"""
		cursor.execute(query)
		gallery = "<gallery>\n"
		for title, gilcount, concat in cursor.fetchall():
				gallery += 'File:'+title +'|<abbr title="'+ concat + '">'+ str(gilcount) +'</abbr>\n'
		gallery += '</gallery>'
		return gallery

def popular_categories(cat_list, badcats):
		if badcats and len(badcats) > 0:
				badcats += USUAL_BADCATS
		else:
				badcats = USUAL_BADCATS
		badcats = '|'.join(list(set(badcats)))  # also gets rid of duplicates
		query="""
SELECT c.cl_to AS category,
COUNT(DISTINCT page_id) AS total
FROM page
INNER JOIN categorylinks AS c ON page_id=c.cl_from AND c.cl_to NOT IN (""" + cat_list +""")
INNER JOIN categorylinks AS cc on page_id=cc.cl_from AND cc.cl_to IN ("""+ cat_list +""")
WHERE c.cl_to NOT REGEXP \"""" + badcats + """\"
GROUP BY c.cl_to
HAVING total>1
ORDER BY COUNT(page_id) DESC
LIMIT 100;
"""
		cursor.execute(query)
		table = "{{flatlist|"
		for cat, total in cursor.fetchall():
				table += "\n*[[:Category:"+re.sub("_"," ",cat)+"|]]"+" ({:,})".format(total)
		table += "\n}}"
		return table

def child_catcher(category, recursive):  # From seed category return children list to 2 levels
		cat = catlib.Category(site, category)
		gen = pagegenerators.SubCategoriesPageGenerator(cat, recurse=recursive)
		children = []
		for c in gen:
				children.append(c.title())
		children.append(category)
		return children

def get_projects():
		source = pywikibot.Page(site, REQUEST_PAGE).get()
		source = source.split('\n==Requests==')[1].split('\n==')[0]
		projects = []
		for p in re.split(r'\n\*[^\*]', source):
				# if re.search('EAGLE', p): continue # DEBUG
				cat = re.sub('\[\[:?|\|.*|\]\]', '', p.split('\n')[0])
				rep = re.sub('^[ \*]*|\[\[:?|\|.*|\]\]', '', p.split('\n')[1])
				if cat[:3]!='Cat': continue
				recursive=0
				badcats = None
				if re.search(r"[Rr]ecursive", " ".join(p.split('\n')[2:])):
						if re.search(r"[Rr]ecursive \d", " ".join(p.split('\n')[2:])):
								recursive=int(float(re.findall(r"[Rr]ecursive (\d)", " ".join(p.split('\n')[2:]))[0]))
								if recursive>6: recursive=6
						else:
								recursive=2
				for row in p.split('\n')[2:]:
						if row.lower().startswith('** badcats: '):
								badcats = row[len('** badcats: '):].strip().replace(' ','_').split('|')
				projects.append([cat, rep, recursive, badcats])
		return projects

def index(bpage):
		bpage = re.sub("_", " ", bpage)
		text = ""
		# Ensure any new reports are added here
		for sub in ["popular categories", "wikimedia usage", "volunteers", "most edited", "largest", "improvement"]:
				text += "* [[:" + bpage + "/" + sub + "|" + sub[0:1].upper() + sub[1:] + "]]\n"
		return text.encode('utf-8', 'ignore')

def put_report(report, spage, action):
		report = report.decode('utf-8', 'ignore')
		page = pywikibot.Page(site, spage)
		try:
				html = page.get()
				if len(html) == len(report):  # There may be some real changes lost, but most of non-length changes are trivial shuffling
						return
		except:
				pass
		page.put(report, comment=action)
		return

def main():
		projects = get_projects()
		for p in range(len(projects)):
				if projects[p][2] == False:
						projects[p][0] = [projects[p][0]]
						continue
				projects[p][0] = child_catcher(projects[p][0], projects[p][2])
		# Make one d/b connection for all queries - more efficient I believe
		conn = MySQLdb.connect(
				read_default_file = "~/replica.my.cnf",
				host = "commonswiki.labsdb",
				db = "commonswiki_p",
                charset = 'utf8'
				)
		global cursor
		cursor = conn.cursor()
		for project in projects:
				cscats = ('"' + '","'.join([re.sub(" ","_",c[9:]) for c in project[0]]) + '"').encode('utf-8')
				put_report(improvement(cscats), project[1] + "/improvement", "GLAM dashboard improvement suggestions")
				put_report(popular_categories(cscats, project[3]), project[1] + "/popular_categories", "GLAM dashboard update popular categories")
				put_report(glamorous_list(cscats), project[1] + "/wikimedia_usage", "GLAM dashboard update usage")
				put_report(volunteers(cscats), project[1] + "/volunteers", "GLAM dashboard update volunteer list")
				put_report(most_edited(cscats), project[1] + "/most_edited", "GLAM dashboard update most edited")
				put_report(largest(cscats), project[1] + "/largest", "GLAM dashboard update largest")
				put_report(index(project[1]), project[1] + "/index", "GLAM dashboard index")
		conn.close()

if __name__ == "__main__":
		usage = u'Usage:\tAsk Fae'
		site = pywikibot.Site('commons', 'commons')
		main()
